{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ«€ Heart Disease Prediction - Complete ML Project\n",
    "\n",
    "## Project Overview\n",
    "Project ini menggunakan dataset Heart Disease untuk memprediksi apakah seseorang memiliki penyakit jantung atau tidak.\n",
    "\n",
    "**Dataset:** Heart Disease Dataset (1025 records, 14 features)\n",
    "\n",
    "**Target:** Binary Classification (0 = No Disease, 1 = Disease)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Preprocessing & Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('heart_disease.csv')\n",
    "\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset info\n",
    "print(\"Dataset Information:\")\n",
    "print(\"=\"*50)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values:\")\n",
    "print(\"=\"*50)\n",
    "missing = df.isnull().sum()\n",
    "print(missing[missing > 0] if missing.sum() > 0 else \"No missing values!\")\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"\\nDuplicate rows: {duplicates}\")\n",
    "\n",
    "if duplicates > 0:\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"Removed {duplicates} duplicate rows\")\n",
    "    print(f\"New shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "print(\"Statistical Summary:\")\n",
    "print(\"=\"*50)\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature descriptions\n",
    "feature_descriptions = {\n",
    "    'age': 'Age of patient (years)',\n",
    "    'sex': 'Gender (1=male, 0=female)',\n",
    "    'cp': 'Chest pain type (0-3)',\n",
    "    'trestbps': 'Resting blood pressure (mm Hg)',\n",
    "    'chol': 'Serum cholesterol (mg/dl)',\n",
    "    'fbs': 'Fasting blood sugar > 120 mg/dl (1=true, 0=false)',\n",
    "    'restecg': 'Resting ECG results (0-2)',\n",
    "    'thalach': 'Maximum heart rate achieved',\n",
    "    'exang': 'Exercise induced angina (1=yes, 0=no)',\n",
    "    'oldpeak': 'ST depression induced by exercise',\n",
    "    'slope': 'Slope of peak exercise ST segment (0-2)',\n",
    "    'ca': 'Number of major vessels (0-4)',\n",
    "    'thal': 'Thalassemia (0-3)',\n",
    "    'target': 'Heart disease (1=yes, 0=no)'\n",
    "}\n",
    "\n",
    "print(\"\\nFeature Descriptions:\")\n",
    "print(\"=\"*50)\n",
    "for feat, desc in feature_descriptions.items():\n",
    "    print(f\"{feat:12} - {desc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Visualization & EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Count plot\n",
    "target_counts = df['target'].value_counts()\n",
    "axes[0].bar(['No Disease', 'Disease'], target_counts.values, color=['#2ecc71', '#e74c3c'])\n",
    "axes[0].set_title('Target Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Count')\n",
    "for i, v in enumerate(target_counts.values):\n",
    "    axes[0].text(i, v + 10, str(v), ha='center', fontweight='bold')\n",
    "\n",
    "# Pie chart\n",
    "axes[1].pie(target_counts.values, labels=['No Disease', 'Disease'], \n",
    "           autopct='%1.1f%%', startangle=90, colors=['#2ecc71', '#e74c3c'])\n",
    "axes[1].set_title('Target Percentage', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Class Balance:\")\n",
    "print(f\"No Disease: {target_counts[0]} ({target_counts[0]/len(df)*100:.1f}%)\")\n",
    "print(f\"Disease: {target_counts[1]} ({target_counts[1]/len(df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age distribution by target\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram\n",
    "for target in [0, 1]:\n",
    "    subset = df[df['target'] == target]['age']\n",
    "    label = 'Disease' if target == 1 else 'No Disease'\n",
    "    axes[0].hist(subset, alpha=0.6, bins=20, label=label)\n",
    "axes[0].set_xlabel('Age')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Age Distribution by Target', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "\n",
    "# Box plot\n",
    "df.boxplot(column='age', by='target', ax=axes[1])\n",
    "axes[1].set_xlabel('Target (0=No Disease, 1=Disease)')\n",
    "axes[1].set_ylabel('Age')\n",
    "axes[1].set_title('Age vs Target', fontsize=14, fontweight='bold')\n",
    "plt.suptitle('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical features analysis\n",
    "categorical_features = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(18, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature in enumerate(categorical_features):\n",
    "    cross_tab = pd.crosstab(df[feature], df['target'], normalize='index') * 100\n",
    "    cross_tab.plot(kind='bar', ax=axes[idx], color=['#2ecc71', '#e74c3c'])\n",
    "    axes[idx].set_title(f'{feature.upper()} vs Target', fontweight='bold')\n",
    "    axes[idx].set_xlabel(feature)\n",
    "    axes[idx].set_ylabel('Percentage (%)')\n",
    "    axes[idx].legend(['No Disease', 'Disease'])\n",
    "    axes[idx].tick_params(axis='x', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuous features distribution\n",
    "continuous_features = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature in enumerate(continuous_features):\n",
    "    for target in [0, 1]:\n",
    "        subset = df[df['target'] == target][feature]\n",
    "        label = 'Disease' if target == 1 else 'No Disease'\n",
    "        axes[idx].hist(subset, alpha=0.6, bins=20, label=label)\n",
    "    axes[idx].set_xlabel(feature)\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "    axes[idx].set_title(f'{feature.upper()} Distribution', fontweight='bold')\n",
    "    axes[idx].legend()\n",
    "\n",
    "# Hide last subplot\n",
    "axes[-1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "plt.figure(figsize=(14, 10))\n",
    "correlation = df.corr()\n",
    "mask = np.triu(np.ones_like(correlation, dtype=bool))\n",
    "sns.heatmap(correlation, mask=mask, annot=True, fmt='.2f', \n",
    "            cmap='coolwarm', center=0, square=True, linewidths=1)\n",
    "plt.title('Correlation Heatmap', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Top correlations with target\n",
    "print(\"\\nTop Correlations with Target:\")\n",
    "print(\"=\"*50)\n",
    "target_corr = correlation['target'].drop('target').sort_values(ascending=False)\n",
    "print(target_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot for key features\n",
    "key_features = ['age', 'trestbps', 'chol', 'thalach', 'target']\n",
    "sns.pairplot(df[key_features], hue='target', palette={0: '#2ecc71', 1: '#e74c3c'})\n",
    "plt.suptitle('Pairplot of Key Features', y=1.02, fontsize=16, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Modelling (Supervised Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    "\n",
    "# Prepare data\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "    'AdaBoost': AdaBoostClassifier(random_state=42),\n",
    "    'SVM': SVC(random_state=42, probability=True),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'Naive Bayes': GaussianNB()\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Train\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    # Cross-validation score\n",
    "    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
    "    cv_mean = cv_scores.mean()\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'ROC-AUC': roc_auc,\n",
    "        'CV Score': cv_mean\n",
    "    })\n",
    "\n",
    "# Results dataframe\n",
    "results_df = pd.DataFrame(results).sort_values('Accuracy', ascending=False)\n",
    "print(\"\\nModel Performance Comparison:\")\n",
    "print(\"=\"*100)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    results_df_sorted = results_df.sort_values(metric)\n",
    "    axes[idx].barh(results_df_sorted['Model'], results_df_sorted[metric])\n",
    "    axes[idx].set_xlabel(metric, fontweight='bold')\n",
    "    axes[idx].set_title(f'Model Comparison - {metric}', fontsize=14, fontweight='bold')\n",
    "    axes[idx].set_xlim([0, 1])\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, v in enumerate(results_df_sorted[metric]):\n",
    "        axes[idx].text(v + 0.01, i, f'{v:.3f}', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model based on accuracy\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "print(f\"\\nðŸ† Best Model: {best_model_name}\")\n",
    "print(\"=\"*50)\n",
    "print(results_df.iloc[0].to_string())\n",
    "\n",
    "# Detailed evaluation of best model\n",
    "y_pred_best = best_model.predict(X_test_scaled)\n",
    "y_pred_proba_best = best_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "print(\"\\n\\nClassification Report:\")\n",
    "print(\"=\"*50)\n",
    "print(classification_report(y_test, y_pred_best, target_names=['No Disease', 'Disease']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Confusion matrix heatmap\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=['No Disease', 'Disease'],\n",
    "            yticklabels=['No Disease', 'Disease'])\n",
    "axes[0].set_title(f'Confusion Matrix - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('True Label')\n",
    "axes[0].set_xlabel('Predicted Label')\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba_best)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba_best)\n",
    "\n",
    "axes[1].plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "axes[1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
    "axes[1].set_xlim([0.0, 1.0])\n",
    "axes[1].set_ylim([0.0, 1.05])\n",
    "axes[1].set_xlabel('False Positive Rate')\n",
    "axes[1].set_ylabel('True Positive Rate')\n",
    "axes[1].set_title(f'ROC Curve - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(loc=\"lower right\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Dimensionality Reduction & Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Feature importance using Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importance (Random Forest):\")\n",
    "print(\"=\"*50)\n",
    "print(feature_importance)\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(feature_importance['Feature'], feature_importance['Importance'])\n",
    "plt.xlabel('Importance Score', fontweight='bold')\n",
    "plt.title('Feature Importance Analysis', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SelectKBest feature selection\n",
    "selector = SelectKBest(score_func=f_classif, k=10)\n",
    "X_train_selected = selector.fit_transform(X_train_scaled, y_train)\n",
    "X_test_selected = selector.transform(X_test_scaled)\n",
    "\n",
    "selected_features = X.columns[selector.get_support()]\n",
    "scores = selector.scores_\n",
    "\n",
    "feature_scores = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Score': scores\n",
    "}).sort_values('Score', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Selection Scores (ANOVA F-value):\")\n",
    "print(\"=\"*50)\n",
    "print(feature_scores)\n",
    "print(f\"\\nSelected Features (Top 10): {list(selected_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA Analysis\n",
    "pca = PCA()\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "# Explained variance\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "# Visualize PCA\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Scree plot\n",
    "axes[0].bar(range(1, len(explained_variance) + 1), explained_variance)\n",
    "axes[0].set_xlabel('Principal Component')\n",
    "axes[0].set_ylabel('Explained Variance Ratio')\n",
    "axes[0].set_title('Scree Plot', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Cumulative variance\n",
    "axes[1].plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o')\n",
    "axes[1].axhline(y=0.95, color='r', linestyle='--', label='95% Variance')\n",
    "axes[1].set_xlabel('Number of Components')\n",
    "axes[1].set_ylabel('Cumulative Explained Variance')\n",
    "axes[1].set_title('Cumulative Variance Explained', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find number of components for 95% variance\n",
    "n_components_95 = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "print(f\"\\nNumber of components for 95% variance: {n_components_95}\")\n",
    "print(f\"Variance explained by {n_components_95} components: {cumulative_variance[n_components_95-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models with different feature sets\n",
    "feature_comparison = []\n",
    "\n",
    "# Original features\n",
    "best_model.fit(X_train_scaled, y_train)\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "acc_original = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Selected features (Top 10)\n",
    "best_model.fit(X_train_selected, y_train)\n",
    "y_pred = best_model.predict(X_test_selected)\n",
    "acc_selected = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# PCA features (95% variance)\n",
    "pca_optimal = PCA(n_components=n_components_95)\n",
    "X_train_pca_optimal = pca_optimal.fit_transform(X_train_scaled)\n",
    "X_test_pca_optimal = pca_optimal.transform(X_test_scaled)\n",
    "best_model.fit(X_train_pca_optimal, y_train)\n",
    "y_pred = best_model.predict(X_test_pca_optimal)\n",
    "acc_pca = accuracy_score(y_test, y_pred)\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Method': ['All Features (13)', f'Selected Features ({len(selected_features)})', f'PCA ({n_components_95} components)'],\n",
    "    'Accuracy': [acc_original, acc_selected, acc_pca]\n",
    "})\n",
    "\n",
    "print(\"\\nFeature Selection Comparison:\")\n",
    "print(\"=\"*50)\n",
    "print(comparison_df)\n",
    "\n",
    "# Visualize comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(comparison_df['Method'], comparison_df['Accuracy'], color=['#3498db', '#2ecc71', '#e74c3c'])\n",
    "plt.ylabel('Accuracy', fontweight='bold')\n",
    "plt.title('Model Performance with Different Feature Sets', fontsize=14, fontweight='bold')\n",
    "plt.ylim([0.7, 1.0])\n",
    "for i, v in enumerate(comparison_df['Accuracy']):\n",
    "    plt.text(i, v + 0.01, f'{v:.4f}', ha='center', fontweight='bold')\n",
    "plt.xticks(rotation=15, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "# We'll tune the best performing model\n",
    "print(f\"Tuning hyperparameters for: {best_model_name}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Define parameter grids for different models\n",
    "param_grids = {\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': ['sqrt', 'log2']\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'subsample': [0.8, 0.9, 1.0],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    },\n",
    "    'Logistic Regression': {\n",
    "        'C': [0.01, 0.1, 1, 10, 100],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear', 'saga']\n",
    "    },\n",
    "    'SVM': {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'gamma': ['scale', 'auto', 0.001, 0.01, 0.1],\n",
    "        'kernel': ['rbf', 'linear']\n",
    "    },\n",
    "    'KNN': {\n",
    "        'n_neighbors': [3, 5, 7, 9, 11],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'metric': ['euclidean', 'manhattan']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Get the appropriate parameter grid\n",
    "if best_model_name in param_grids:\n",
    "    param_grid = param_grids[best_model_name]\n",
    "else:\n",
    "    param_grid = {}\n",
    "    print(f\"No predefined parameter grid for {best_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if param_grid:\n",
    "    # Perform Grid Search\n",
    "    print(\"\\nPerforming Grid Search...\")\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=best_model,\n",
    "        param_grid=param_grid,\n",
    "        cv=5,\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    print(\"\\n\\nBest Parameters:\")\n",
    "    print(\"=\"*50)\n",
    "    for param, value in grid_search.best_params_.items():\n",
    "        print(f\"{param}: {value}\")\n",
    "    \n",
    "    print(f\"\\nBest Cross-Validation Score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Evaluate tuned model\n",
    "    tuned_model = grid_search.best_estimator_\n",
    "    y_pred_tuned = tuned_model.predict(X_test_scaled)\n",
    "    \n",
    "    print(\"\\n\\nTuned Model Performance:\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred_tuned):.4f}\")\n",
    "    print(f\"Precision: {precision_score(y_test, y_pred_tuned):.4f}\")\n",
    "    print(f\"Recall: {recall_score(y_test, y_pred_tuned):.4f}\")\n",
    "    print(f\"F1-Score: {f1_score(y_test, y_pred_tuned):.4f}\")\n",
    "else:\n",
    "    tuned_model = best_model\n",
    "    print(\"Using original best model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare before and after tuning\n",
    "if param_grid:\n",
    "    comparison = pd.DataFrame({\n",
    "        'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score'],\n",
    "        'Before Tuning': [\n",
    "            accuracy_score(y_test, y_pred_best),\n",
    "            precision_score(y_test, y_pred_best),\n",
    "            recall_score(y_test, y_pred_best),\n",
    "            f1_score(y_test, y_pred_best)\n",
    "        ],\n",
    "        'After Tuning': [\n",
    "            accuracy_score(y_test, y_pred_tuned),\n",
    "            precision_score(y_test, y_pred_tuned),\n",
    "            recall_score(y_test, y_pred_tuned),\n",
    "            f1_score(y_test, y_pred_tuned)\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    comparison['Improvement'] = comparison['After Tuning'] - comparison['Before Tuning']\n",
    "    \n",
    "    print(\"\\nPerformance Comparison:\")\n",
    "    print(\"=\"*50)\n",
    "    print(comparison)\n",
    "    \n",
    "    # Visualize comparison\n",
    "    comparison.set_index('Metric')[['Before Tuning', 'After Tuning']].plot(\n",
    "        kind='bar', figsize=(10, 6), rot=0\n",
    "    )\n",
    "    plt.title('Model Performance: Before vs After Tuning', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Score')\n",
    "    plt.ylim([0.7, 1.0])\n",
    "    plt.legend(['Before Tuning', 'After Tuning'])\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Model Deployment Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the best tuned model\n",
    "final_model = tuned_model if param_grid else best_model\n",
    "\n",
    "with open('heart_disease_model.pkl', 'wb') as f:\n",
    "    pickle.dump(final_model, f)\n",
    "\n",
    "# Save the scaler\n",
    "with open('scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print(\"Model saved: heart_disease_model.pkl\")\n",
    "print(\"Scaler saved: scaler.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prediction function\n",
    "def predict_heart_disease(age, sex, cp, trestbps, chol, fbs, restecg, \n",
    "                         thalach, exang, oldpeak, slope, ca, thal):\n",
    "    \"\"\"\n",
    "    Predict heart disease based on patient features\n",
    "    \n",
    "    Returns:\n",
    "        prediction: 0 (No Disease) or 1 (Disease)\n",
    "        probability: Probability of having heart disease\n",
    "    \"\"\"\n",
    "    # Create feature array\n",
    "    features = np.array([[age, sex, cp, trestbps, chol, fbs, restecg,\n",
    "                         thalach, exang, oldpeak, slope, ca, thal]])\n",
    "    \n",
    "    # Scale features\n",
    "    features_scaled = scaler.transform(features)\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = final_model.predict(features_scaled)[0]\n",
    "    probability = final_model.predict_proba(features_scaled)[0][1]\n",
    "    \n",
    "    return prediction, probability\n",
    "\n",
    "# Test the function\n",
    "print(\"Testing prediction function:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Example patient\n",
    "test_patient = X_test.iloc[0]\n",
    "pred, prob = predict_heart_disease(*test_patient.values)\n",
    "\n",
    "print(f\"Patient features: {test_patient.to_dict()}\")\n",
    "print(f\"\\nPrediction: {'Disease' if pred == 1 else 'No Disease'}\")\n",
    "print(f\"Probability of disease: {prob:.2%}\")\n",
    "print(f\"Actual: {'Disease' if y_test.iloc[0] == 1 else 'No Disease'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\\n\n",
    "          HEART DISEASE PREDICTION - PROJECT SUMMARY           \n",
    "\n",
    "DATASET OVERVIEW:\n",
    "   â€¢ Total Records: 1025\n",
    "   â€¢ Features: 13 clinical measurements\n",
    "   â€¢ Target: Binary classification (Disease/No Disease)\n",
    "   â€¢ Class Distribution: Balanced dataset\n",
    "\n",
    "KEY FINDINGS:\n",
    "   1. Most Important Features:\n",
    "      - Number of major vessels (ca)\n",
    "      - Chest pain type (cp)\n",
    "      - Maximum heart rate (thalach)\n",
    "      - ST depression (oldpeak)\n",
    "   \n",
    "   2. Strong Correlations with Target:\n",
    "      - cp, thalach, slope (positive)\n",
    "      - exang, oldpeak, ca (negative)\n",
    "\n",
    "MODEL PERFORMANCE:\n",
    "\"\"\")\n",
    "\n",
    "print(f\"   â€¢ Best Model: {best_model_name}\")\n",
    "print(f\"   â€¢ Test Accuracy: {accuracy_score(y_test, y_pred_tuned if param_grid else y_pred_best):.2%}\")\n",
    "print(f\"   â€¢ Precision: {precision_score(y_test, y_pred_tuned if param_grid else y_pred_best):.2%}\")\n",
    "print(f\"   â€¢ Recall: {recall_score(y_test, y_pred_tuned if param_grid else y_pred_best):.2%}\")\n",
    "print(f\"   â€¢ F1-Score: {f1_score(y_test, y_pred_tuned if param_grid else y_pred_best):.2%}\")\n",
    "\n",
    "print(\"\"\"\\n\n",
    "DIMENSIONALITY REDUCTION:\n",
    "   â€¢ Feature Selection: Top 10 features maintain high accuracy\n",
    "   â€¢ PCA: {n_components_95} components explain 95% variance\n",
    "   â€¢ Conclusion: All features contribute to predictions\n",
    "\n",
    "DEPLOYMENT:\n",
    "   â€¢ Model saved as: heart_disease_model.pkl\n",
    "   â€¢ Scaler saved as: scaler.pkl\n",
    "   â€¢ Ready for Streamlit deployment\n",
    "\n",
    "RECOMMENDATIONS:\n",
    "   1. Monitor patients with high ca, oldpeak values\n",
    "   2. Chest pain type (cp) is a strong early indicator\n",
    "   3. Combination of multiple factors gives best predictions\n",
    "   4. Model can assist in early screening and risk assessment\n",
    "\n",
    "      COMPLETE\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
